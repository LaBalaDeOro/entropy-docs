---
sidebar_position: 1
---

# Introduction

Entropy is a physical phenomenon characterized by a curious law of physics:

**There may only be more of it.**

This is a restatement of the second law of thermodynamics, which formally states that *the total entropy of any isolated thermodynamic system tends to increase over time, approaching a maximum value*. The implication is that, since the Universe is assumed to be an isolated system, it is safe to say that the total entropy contained within the Universe is always increasing.

An unspoken assumption behind the second law is that the Universe has no mind of its own and the atoms that comprise it follow pre-determined physical laws tending towards average behavior. If one accepts this assumption, then one must accept that the Universe is in a constant state of decay against which there is not much that we can do. Moreover, the existence of our own intelligence is only but an anomalous blip in the passage of space and time.

We have just described the concept of thermodynamic, or physical, entropy. An analogous concept is that of information entropy, which measures the randomness contained in a piece of information. The link between physical entropy and information entropy becomes clearer if we notice that the natural tendency of information is to become less coherent without upkeep from the outside: for example, memories fade, books are lost when no one reads them, and skill declines without practice.

**Enter ENTROPY.**

ENTROPY is a web3 project that embraces the awesome power of randomness. In service thereof, ENTROPY has founded itself as DePIN (decentralized physical infrastructure network) that converts thermodynamic entropy into information entropy and provides proof of this conversion on chain.
